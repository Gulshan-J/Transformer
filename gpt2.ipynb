{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer,GPT2Config\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.32.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config=GPT2Config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(10,10)) # for masked attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt2attn(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(gpt2attn, self).__init__()\n",
    "        \n",
    "        max_postion=config.n_positions\n",
    "        self.mask=torch.tril(torch.ones(max_postion,max_postion),dtype=torch.uint8).unsqueeze(0).unsqueeze(0)\n",
    "        # or below\n",
    "        # self.mask=torch.tril(torch.ones(max_postion,max_postion),dtype=torch.uint8).view(1,1,max_postion,max_postion)\n",
    "        self.embed_dim=config.n_embed\n",
    "        self.head=config.n_head\n",
    "        self.head_no=self.embed_dim//self.head\n",
    "        self.split_size=self.embed_dim\n",
    "        self.c_attn=nn.Linear(self.embed_dim,3*self.embed_dim) # for q,k,v\n",
    "        self.c_proj=nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "    \n",
    "    def _attn(self,q,k,v):\n",
    "        # shape of q,k,v -> (b,n_head,seq_len,d_head)\n",
    "        attn_weight=torch.matmul(q,k.transpose(-1,-2))\n",
    "        attn_weight/=float(q.size(-1))**0.5\n",
    "        seq_len=q.size(-2)\n",
    "        causal_mask=self.mask[:,:,:seq_len,:seq_len].bool()\n",
    "        attn_weight=torch.where(causal_mask,attn_weight,torch.tensor(-1e4))\n",
    "        attn_weight=F.softmax(attn_weight,dim=-1)\n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "        #attn_weight : (b,n_head,seq_len,sqe_len)\n",
    "        # we dont need to transpose as it can be matrix mulltipilcablke\n",
    "        attn_out=torch.matmul(attn_weight,v)\n",
    "        \n",
    "        return attn_out\n",
    "    def forward(self,x):\n",
    "        # shape of x -> batch,seq_len,dim_model\n",
    "        b,seq_len,dim_model=x\n",
    "        q,k,v=self.c_attn(x).split(self.split_size,dim=-1) # (Batch,seq_len,c*3) #split size =c\n",
    "        q=q.view(b,seq_len,self.head,self.head_no).transpose(1,2)\n",
    "        k=k.view(b,seq_len,self.head,self.head_no).transpose(1,2)\n",
    "        v=v.view(b,seq_len,self.head,self.head_no).transpose(1,2)\n",
    "        # shape (b,n_head,seq_len,d_head)\n",
    "        \n",
    "        attn_out=self._attn(q,k,v)\n",
    "        attn_out=attn_out.transpose(1,2).view(b,seq_len,dim_model)\n",
    "        attn_out=self.dropout(attn_out)\n",
    "        \n",
    "        return attn_out  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt2mlp(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super(gpt2mlp,self).__init__()\n",
    "        embed_dim=config.n_embed\n",
    "        self.mlp=nn.Sequential(nn.Linear(embed_dim,4*embed_dim),\n",
    "                               nn.GELU(),\n",
    "                               nn.Linear(4*embed_dim,embed_dim),\n",
    "                               nn.Dropout(0.1))\n",
    "    def forward(Self,x):\n",
    "        return self.mlp(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt2block(nn.Module):\n",
    "    def __init__(self,config) -> None:\n",
    "        super(gpt2block,self).__init__()\n",
    "        embed_dim=config.n_embed\n",
    "        self.layer_norm_1=nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm_2=nn.LayerNorm(embed_dim)\n",
    "        self.attn=gpt2attn(config)\n",
    "        self.mlp=gpt2mlp(config)\n",
    "        \n",
    "    def forward(self,hidden_states):\n",
    "        residual=hidden_states\n",
    "        hidden_states=self.layer_norm_1(hidden_states)\n",
    "        attn_out=self.attn(hidden_states)\n",
    "        hidden_states=attn_out+residual\n",
    "        residual=hidden_states\n",
    "        ffw_hidden_states=self.mlp(hidden_states)\n",
    "        hidden_states=residual+ffw_hidden_states\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt2model(nn.Module):\n",
    "    def __init__(self,config) -> None:\n",
    "        super(gpt2model,self).__init__()\n",
    "        self.embed_dim=config.n_embed\n",
    "        self.vocab_size=config.vocab_size\n",
    "        self.token_embed=nn.Embedding(self.vocab_size,self.embed_dim)\n",
    "        self.postional_embed=nn.Embedding(config.n_positions,self.embed_dim)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.1)\n",
    "        self.blocks=nn.ModuleList([gpt2block(config) for _ in range(config.n_layer)])\n",
    "        \n",
    "        self.layer_norm_final=nn.LayerNorm(self.embed_dim)\n",
    "        \n",
    "    def forward(self,input_ids=None,position_ids=None,):\n",
    "        # input ids (batch size,max seq len)\n",
    "        input_shape=input_ids.size()\n",
    "        batch_size=input_ids.size(0)\n",
    "        device=input_ids.device\n",
    "        \n",
    "        position_ids=torch.arange(0,input_ids.size(-1),dtype=torch.long,device=device).unsqueeze(0)\n",
    "        \n",
    "        input_embeds=self.token_embed(input_ids)\n",
    "        position_embed=self.postional_embed(position_ids)\n",
    "        \n",
    "        hidden_states=input_embeds+position_embed\n",
    "        hidden_states=self.drop(hidden_states)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            hidden_states=block(hidden_states)\n",
    "            \n",
    "        hidden_states=self.layer_norm_final(hidden_states)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0,10) #-> tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt2languagehead(nn.Module):\n",
    "    def __init__(self,config) -> None:\n",
    "        super(gpt2languagehead,self).__init__()\n",
    "        self.embed_dim=config.n_embed\n",
    "        self.transfomer=gpt2model(config)\n",
    "        self.lm_head=nn.Linear(config.n_embed,config.vocab_size,bias=False)\n",
    "        self.xe=nn.CrossEntropyLoss(ignore_index=GPT2Tokenizer.pad_token)\n",
    "    \n",
    "    def forward(self,input_ids=None,position_ids=None,labels=None):\n",
    "        \n",
    "        hidden_states=self.transfomer(input_ids)\n",
    "        lm_head=self.lm_head(hidden_states)# (bs,max seq len ,vocab size)\n",
    "        \n",
    "        loss= None\n",
    "        if labels is not None:\n",
    "            # <bos> hey dude !\n",
    "            # labels -> hey dude ! <eos>\n",
    "            shift_logits=lm_head[:,:-1,:]\n",
    "            shift_label=labels[:,1:]\n",
    "            loss=self.xe(shift_logits.view(-1,shift_logits.size(-1)),shift_label.view(-1))\n",
    "            \n",
    "        return lm_head,loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gulshan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
